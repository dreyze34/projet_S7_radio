{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, img_size):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.flatten_dim = img_size * img_size\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        assert height == self.img_size and width == self.img_size, \"Input image size must match transformer input size.\"\n",
    "\n",
    "        x = x.flatten(2).permute(2, 0, 1)  # (batch, channels, height*width) -> (height*width, batch, channels)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 2, 0).reshape(batch_size, channels, height, width)  # Restore shape\n",
    "\n",
    "        return x\n",
    "\n",
    "class UNetWithTransformer(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, base_channels=64, embed_dim=256, num_heads=8, num_layers=4, img_size=128):\n",
    "        super(UNetWithTransformer, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_channels, base_channels)\n",
    "        self.enc2 = ConvBlock(base_channels, base_channels * 2)\n",
    "        self.enc3 = ConvBlock(base_channels * 2, base_channels * 4)\n",
    "        self.enc4 = ConvBlock(base_channels * 4, base_channels * 8)\n",
    "        \n",
    "        # Transformer block\n",
    "        self.transformer = TransformerBlock(embed_dim, num_heads, num_layers, img_size // 16)\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Decoder\n",
    "        self.up3 = nn.ConvTranspose2d(base_channels * 8, base_channels * 4, kernel_size=2, stride=2)\n",
    "        self.dec3 = ConvBlock(base_channels * 8, base_channels * 4)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2)\n",
    "        self.dec2 = ConvBlock(base_channels * 4, base_channels * 2)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=2, stride=2)\n",
    "        self.dec1 = ConvBlock(base_channels * 2, base_channels)\n",
    "        \n",
    "        self.final = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "\n",
    "        # Transformer\n",
    "        bottleneck = self.pool(enc4)\n",
    "        bottleneck = self.transformer(bottleneck)\n",
    "\n",
    "        # Decoder\n",
    "        dec3 = self.dec3(torch.cat([self.up3(bottleneck), enc3], dim=1))\n",
    "        dec2 = self.dec2(torch.cat([self.up2(dec3), enc2], dim=1))\n",
    "        dec1 = self.dec1(torch.cat([self.up1(dec2), enc1], dim=1))\n",
    "        \n",
    "        return self.final(dec1)\n",
    "\n",
    "# Exemple d'initialisation du modèle\n",
    "model = UNetWithTransformer(\n",
    "    in_channels=1, \n",
    "    out_channels=1, \n",
    "    base_channels=64, \n",
    "    embed_dim=256, \n",
    "    num_heads=8, \n",
    "    num_layers=4, \n",
    "    img_size=128\n",
    ")\n",
    "\n",
    "# Affichage de la structure du modèle\n",
    "print(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
